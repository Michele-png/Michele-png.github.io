{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Setup Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile, Levenshtein, pycountry, re, json\n",
    "from textdistance import jaro_winkler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing and Elaborating Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pBook Tab \n",
    "path = '.\\Dropbox\\Patent name match\\comlist_forpatent.xlsx' \n",
    "cols = ['UID', 'CompanyName', 'Country/Region']\n",
    "df_firms = pd.read_excel(path, usecols=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shorten Firms Country/Region\n",
    "Note on Missing Values Refill for Firms Country/Region: \n",
    "- Firm Bygg had state California, but Country/Region empty, it was manually filled in the excel sheet as United States; \n",
    "- CELL AGRITECH SDN BHD had Country/Region empty, a quick google search revealts they are headquartered in Malaysia; \n",
    "- Health Sources Nutrition Co.,Ltd. had no Coutry/Region, but its headquarters are reported to be in Heifei, i.e. Mainland China. \n",
    "\n",
    "These changes were made directly in the source table i.e. pBook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "country_name_to_code = {country.name: country.alpha_2 for country in pycountry.countries}\n",
    "mapping_missing_countries = {'Czech Republic' : 'CZ',\n",
    "                        'India ' : 'IN',\n",
    "                        'Mainland China' : 'CN',\n",
    "                        'Russia' : 'RU',\n",
    "                        'Scotland' : 'GB',\n",
    "                        'South Korea' : 'KR',\n",
    "                        'Taiwan' : 'TW',\n",
    "                        'Venezuela' : 'VE',\n",
    "                        'Vietnam' : 'VN',\n",
    "                        'latvia' : 'LV'}\n",
    "                        \n",
    "country_name_to_code.update(mapping_missing_countries)\n",
    "df_firms['Country/Region'] = df_firms['Country/Region'].map(country_name_to_code)\n",
    "df_firms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract People\n",
    "path_people1 = '.\\Dropbox\\Patent name match\\Patstat2021a\\data_PATSTAT_Global_2021_Autumn\\tls206_part01.zip'\n",
    "csv_people1 = 'tls206_part01.csv'\n",
    "path_people2 = '.\\Dropbox\\Patent name match\\Patstat2021a\\data_PATSTAT_Global_2021_Autumn\\tls206_part02.zip'\n",
    "csv_people2 = 'tls206_part02.csv'\n",
    "\n",
    "cols_people = ['person_id', 'person_name', 'person_ctry_code', 'psn_sector']\n",
    "\n",
    "with zipfile.ZipFile(path_people1, 'r') as z:\n",
    "    with z.open(csv_people1) as f:\n",
    "            d_people1 = pd.read_csv(f, nrows = None, usecols=cols_people)\n",
    "        \n",
    "with zipfile.ZipFile(path_people2, 'r') as z:\n",
    "    with z.open(csv_people2) as f:\n",
    "            d_people2 = pd.read_csv(f, nrows = None, usecols=cols_people)\n",
    "\n",
    "# concatenate \n",
    "d_people = pd.concat([d_people1, d_people2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter People for Applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a better method to obtain a list of applicants\n",
    "df_applicants = d_people.loc[d_people['psn_sector']=='COMPANY', d_people.columns!='psn_sector']\n",
    "df_applicants.rename(columns={'person_id':'applicant_id', 'person_name':'applicant_name', 'person_ctry_code':'applicant_ctry_code'}, inplace=True)\n",
    "df_applicants.reset_index(inplace=True, drop=True)\n",
    "del d_people, d_people1, d_people2\n",
    "df_applicants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Missing-Management and Standardizations of Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal Applicants Country Code in PATSTAT\n",
    "Recall that there is no need to harmonize country code in PATSTAT. \n",
    "We could have to refill person name as it sometimes is missing, and we could have rather than nan coutry code, empyty coutry code in the shape of '  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many applicants have empty coutry code\n",
    "n = df_applicants['applicant_ctry_code'].isna().sum()\n",
    "k = df_applicants.loc[df_applicants['applicant_ctry_code'] == \"  \", :].count()[0]\n",
    "print(f'there are {df_applicants.shape[0]} applicants and {n} of them have the coutry code missing')\n",
    "print(f'there are {df_applicants.shape[0]} applicants and {k} of them have the coutry code empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whenever the coutry code is missing, replace it with XX\n",
    "df_applicants.loc[df_applicants['applicant_ctry_code']=='  ', 'applicant_ctry_code'] = 'XX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardize Companies and Firms Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the standardization dictionary from the JSON file\n",
    "with open(\"standardization_dict.json\", \"r\") as json_file:\n",
    "    standardization_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_names(name):\n",
    "\n",
    "    try:\n",
    "        name = re.sub(r'[^\\w\\s]', '', name.upper()) #leave only numbers and lettersn turn characters upper\n",
    "        for key, val in standardization_dict.items(): #loop over legal designations and their corresponding WOS structure\n",
    "            key, val = key.strip(), val.strip()\n",
    "            name = name.replace(key, val)\n",
    "        return name.strip()  # Strip any leading/trailing spaces\n",
    "    except:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Edison, Gas UNIVERSITIES 15/12 Av.'\n",
    "standardize_names(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear names in both databases\n",
    "df_firms['Clear Name Firm'] = (df_firms['CompanyName'].apply(standardize_names)).copy(deep=True)\n",
    "df_applicants['Clear Name Applicant'] = (df_applicants['applicant_name'].apply(standardize_names)).copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fuzzy Matching of pBook Firms and Patstat Applicant Company Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use alphanumeric matching on names without legal designation\n",
    "df_perfect = pd.merge(df_applicants, df_firms, how='inner', left_on='Clear Name Applicant', right_on='Clear Name Firm')\n",
    "df_perfect['Match Type'] = 'Alphanumeric'\n",
    "df_perfect['Score'] = np.nan\n",
    "df_perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Broadcasted JW Distance Implementation\n",
    "\n",
    "# Set threshold for the Jaro Winkler score of two names. If their score exceeds the threshold, they are considered equal. \n",
    "threshold = 0.935\n",
    "# vectorize the pyfunction get_jaro_distance so that it can take as input arrays of the same shape. \n",
    "vectorized_JW = np.vectorize(jaro_winkler)\n",
    "\n",
    "# Create the ndarrays on which to apply vectorized_JW\n",
    "firms_names = df_firms['Clear Name Firm'].values\n",
    "applicants_names = df_applicants['Clear Name Applicant'].values\n",
    "applicants_names = np.reshape(applicants_names, (-1, 1))\n",
    "broadcasted_firms_names, broadcasted_applicants_names = np.broadcast_arrays(firms_names, applicants_names)\n",
    "# Apply get_jaro_distance on each couple of names from firms on the horizontal axis, and applicants on the vertical axis. \n",
    "# Save in JW whether the result exceeded the threshold or not. \n",
    "JW = vectorized_JW(broadcasted_firms_names, broadcasted_applicants_names)\n",
    "\n",
    "# Place the firms index as columns of the resulting boolean dataframe, and the applicant index as its index. Thus create a DataFrame.\n",
    "# Then stack it and reset the index as to obtain a form [applicant_index][firm_index][Jaro-Winkler score of the names at those positions]\n",
    "JW = pd.DataFrame(JW, index=df_applicants['Clear Name Applicant'].index, columns=df_firms['Clear Name Firm'].index)\n",
    "JW = JW.stack()\n",
    "JW = JW.reset_index()\n",
    "JW.columns = ['applicant_index', 'firm_index', 'JW_score']\n",
    "\n",
    "# Retreive the couples of applicant and firm indeces which scored above the threshold, store them in filtering masks. Then, apply these marsks on the original\n",
    "# firm and applicant dataframes to recover DataFrames with full rows the names of which exceeded the threshold. Concatenate them,  insert Match Type = Jaro-Winkler,\n",
    "# and insert the JW score of the corresponding names\n",
    "index_mask = JW.loc[(JW['JW_score']>threshold), ['applicant_index', 'firm_index']] \n",
    "applicants_to_append = (df_applicants.loc[index_mask['applicant_index']]).reset_index(drop=True)\n",
    "firms_to_append = (df_firms.loc[index_mask['firm_index']]).reset_index(drop=True)\n",
    "df_JW = pd.concat([applicants_to_append, firms_to_append], axis=1)\n",
    "df_JW['Match Type'] = 'Jaro-Winkler'\n",
    "df_JW['Score'] = JW.loc[index_mask.index, 'JW_score'].values\n",
    "\n",
    "df_JW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Broadcasted LV Distance Implementation\n",
    "\n",
    "# Set threshold for the Levenstein score of two names, which is the max number of character changes between two strings for them to be considered equal\n",
    "threshold = 2\n",
    "# vectorize the pyfunction get_jaro_distance so that it can take as input arrays of the same shape. \n",
    "vectorized_LV = np.vectorize(Levenshtein.distance)\n",
    "\n",
    "# Create the arrays on which to apply vectorized_LV\n",
    "firms_names = df_firms['Clear Name Firm'].values\n",
    "applicants_names = df_applicants['Clear Name Applicant'].values\n",
    "applicants_names = np.reshape(applicants_names, (-1, 1))\n",
    "broadcasted_firms_names, broadcasted_applicants_names = np.broadcast_arrays(firms_names, applicants_names)\n",
    "# Apply Levenstein distance on each couple of names from firms on the horizontal axis, and applicants on the vertical axis. \n",
    "# Save in LV whether the result is below the threshold or not. \n",
    "LV = vectorized_LV(broadcasted_firms_names, broadcasted_applicants_names)\n",
    "\n",
    "# Place the firms index as columns of the resulting boolean dataframe, and the applicant index as its index. Thus create a DataFrame.\n",
    "# Then stack it and reset the index as to obtain a form [applicant_index][firm_index][boolean of whether the names at those positions are below the threshold]\n",
    "LV = pd.DataFrame(LV, index=df_applicants['Clear Name Applicant'].index, columns=df_firms['Clear Name Firm'].index)\n",
    "LV = LV.stack()\n",
    "LV = LV.reset_index()\n",
    "LV.columns = ['applicant_index', 'firm_index', 'LV_score']\n",
    "\n",
    "# Retreive the couples of applicant and firm indeces which scored true, store them in filtering masks. Then, apply these marsks on the original\n",
    "# firm and applicant dataframes to recover DataFrames with full rows the names of which exceeded the threshold. Concatenate them and insert Match Type = Levenshtein. \n",
    "index_mask = LV.loc[(LV['LV_score']<threshold), ['applicant_index', 'firm_index']] \n",
    "applicants_to_append = (df_applicants.loc[index_mask['applicant_index']]).reset_index(drop=True)\n",
    "firms_to_append = (df_firms.loc[index_mask['firm_index']]).reset_index(drop=True)\n",
    "df_LV = pd.concat([applicants_to_append, firms_to_append], axis=1)\n",
    "df_LV['Match Type'] = 'Levenshtein'\n",
    "df_LV['Score'] = LV.loc[index_mask.index, 'LV_score'].values\n",
    "\n",
    "df_LV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the resulting tables togerther\n",
    "df_fuzzy = pd.concat([df_perfect, df_JW, df_LV], axis = 0)\n",
    "df_fuzzy = df_fuzzy.loc[:, (df_fuzzy.columns != 'Clear Name Applicant')&(df_fuzzy.columns != 'Clear Name Firm')]\n",
    "df_fuzzy.reset_index(drop=True, inplace=True)\n",
    "df_fuzzy.drop_duplicates(inplace=True)\n",
    "df_fuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Additional Filters and Disambiguations\n",
    "Use country code to disambiguate between rows with double matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Quality of Location Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_quality(applicant_ctry, firm_ctry):\n",
    "    if (applicant_ctry == 'XX') or (firm_ctry == 'XX'):\n",
    "        return 'Inconclusive'\n",
    "    elif (applicant_ctry == firm_ctry):\n",
    "        return 'Matched'\n",
    "    elif (applicant_ctry != firm_ctry):\n",
    "        return 'Not Matched'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuzzy['Location Match'] = df_fuzzy.apply(lambda row: match_quality(row['applicant_ctry_code'], row['Country/Region']), axis=1)\n",
    "df_fuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disambiguate on Goodness of Name and Location Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create optimality metric as the cartesian distance of the match (Type, Location) score from (3, 3) which is the maximum attainable.\n",
    "d_loc = {'Not Matched': 1, 'Inconclusive':2, 'Matched':3}\n",
    "d_type = {'Levenshtein':1, 'Jaro-Winkler':2, 'Alphanumeric':3}\n",
    "disamb_df_fuzzy = df_fuzzy.copy()\n",
    "disamb_df_fuzzy['opt_distance'] = np.sqrt((disamb_df_fuzzy['Location Match'].map(d_loc))**2 + (disamb_df_fuzzy['Match Type'].map(d_type))**2)\n",
    "\n",
    "# Ensure that one company maps exactly on one applicant, then that one applicant maps exactly on one comapny. \n",
    "# Note that idmin() will return the index of the first occurrence of the minimum value, with no tie handling\n",
    "for col in tqdm(['UID', 'applicant_id']):\n",
    "    idx_min_rows = disamb_df_fuzzy.groupby(col)['opt_distance'].idxmin()\n",
    "    disamb_df_fuzzy = disamb_df_fuzzy.loc[idx_min_rows]\n",
    "\n",
    "# Display the result\n",
    "disamb_df_fuzzy.drop('opt_distance', axis=1, inplace=True)\n",
    "disamb_df_fuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Opportunities for Further Elaborations\n",
    "Use the threshold to filter, then the country code, then filter on industry information. \n",
    "Look at IPC class in patstat, it's not industry but they are activities related to the industry, so that you know food companies would only patent in certain classes \n",
    "and biotech companies would only patent in those other classes. \n",
    "Identify IPC classes in which food & beverage firms would make a patent, but the name and the courty code is good enough for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Final Files Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final output:\n",
    "\n",
    "The output should be a table with CrunchBase company ID, name, country, year of fundation and PATSTAT applicant ID, name, address country. Then it should include the type of match, and the filtering criterion.\n",
    "\n",
    "\n",
    "1. Add a column to the excel file where you identify whether the firm has a patent or not. Write type of match, score of match, whether coutry and region was correct. \n",
    "2. For those with a patent, the output should look like the sample patent shared with me - patent number, applicant ID etc etc. that's the real final file. \n",
    "3. Make the four output files in the examples. \n",
    "4. write what you did and why for a person who does not know much about programming. This can be done after hadning in the files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Columns to pBook File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import pBook File\n",
    "path = '.\\Dropbox\\Patent name match\\comlist_forpatent.xlsx' \n",
    "df_firms = pd.read_excel(path)\n",
    "df_firms\n",
    "\n",
    "# Perform a Left Join between pBook and disamb_df_fuzzy on UID\n",
    "right_merge = disamb_df_fuzzy.loc[:, ['UID', 'applicant_id', 'Match Type', 'Score', 'Location Match']]\n",
    "df_paste = pd.merge(df_firms, right_merge, on='UID', how='left')\n",
    "df_paste.rename(columns={'applicant_id':'Matched'}, inplace=True)\n",
    "df_paste['Matched'] = (~df_paste['Matched'].isna())\n",
    "df_paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write resulting dataframe to the output sheet of the pBook excel\n",
    "path = '.\\Dropbox\\Patent name match\\Process Code - Michele\\comlist_forpatent result version.xlsx'\n",
    "sheet_name = 'Output Sheet'  # Replace with the desired sheet name\n",
    "\n",
    "# Write the DataFrame to the specified sheet\n",
    "with pd.ExcelWriter(path, engine='openpyxl', mode='w') as writer:\n",
    "    df_paste.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce Output pbookappln_ipc\n",
    "This table must have as headers the ID of an application, the symbol of its IPC class, and the count of how often that symbol appears in the database. All this data is found in  tls209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Internatonal Patent Classification Tab\n",
    "path_class1 = '.\\Dropbox\\Patent name match\\Patstat2021a\\data_PATSTAT_Global_2021_Autumn\\tls209_part01.zip'\n",
    "csv_class1 = 'tls209_part01.csv'\n",
    "path_class2 = '.\\Dropbox\\Patent name match\\Patstat2021a\\data_PATSTAT_Global_2021_Autumn\\tls209_part02.zip'\n",
    "csv_class2 = 'tls209_part02.csv'\n",
    "\n",
    "cols_people = ['appln_id', 'ipc_class_symbol']\n",
    "\n",
    "with zipfile.ZipFile(path_class1, 'r') as z:\n",
    "    with z.open(csv_class1) as f:\n",
    "            d_class1 = pd.read_csv(f, nrows=None, usecols=cols_people)\n",
    "        \n",
    "with zipfile.ZipFile(path_class2, 'r') as z:\n",
    "    with z.open(csv_class2) as f:\n",
    "            d_class2 = pd.read_csv(f, nrows=None, usecols=cols_people)\n",
    "\n",
    "# concatenate \n",
    "d_class = pd.concat([d_class1, d_class2], axis=0)\n",
    "del d_class1, d_class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the procedure over the entire table\n",
    "pbookappln_ipc = d_class.groupby('ipc_class_symbol').count()\n",
    "pbookappln_ipc.rename(columns={'appln_id' : 'count'}, inplace=True)\n",
    "pbookappln_ipc = pd.merge(d_class, pbookappln_ipc, how='left', left_on='ipc_class_symbol', right_index=True)\n",
    "\n",
    "# Specify the desktop folder and save the DataFrame to a CSV file\n",
    "desktop_folder = '.\\Dropbox\\Patent name match\\Process Code - Michele'  \n",
    "csv_file_path = f'{desktop_folder}/pbookapplnipc.csv'\n",
    "pbookappln_ipc.to_csv(csv_file_path, index=False)\n",
    "pbookappln_ipc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce Output pbookappln_ipc35\n",
    "The second and third position in the IPC_CLASS_SYMBOL string represent the application macro class from 01 to 99. I will retreive application ID, specify which IPC macro class they belong to - if it is withing the first 35. Then, outline the fraction of applicaitons in that macroclass with respect to all the applicaitons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the procedure over the entire database\n",
    "wdf = d_class.copy(deep=True)\n",
    "wdf['IPC_macro_class'] = wdf['ipc_class_symbol'].apply(lambda s: int(s[1:3]))\n",
    "wdf = wdf.groupby('IPC_macro_class').count()/wdf.shape[0]\n",
    "wdf.rename(columns={'appln_id':'frequency'}, inplace=True)\n",
    "wdf.reset_index(inplace=True)\n",
    "wdf = wdf.loc[wdf['IPC_macro_class']<=35, ['IPC_macro_class', 'frequency']]\n",
    "\n",
    "pbookappln_ipc35 = d_class\n",
    "pbookappln_ipc35['IPC_macro_class'] = pbookappln_ipc35['ipc_class_symbol'].apply(lambda s: int(s[1:3]))\n",
    "pbookappln_ipc35 = pd.merge(pbookappln_ipc35, wdf, how='left', on='IPC_macro_class')\n",
    "pbookappln_ipc35 = pbookappln_ipc35.loc[:, pbookappln_ipc35.columns!='ipc_class_symbol']\n",
    "pbookappln_ipc35.drop_duplicates(inplace=True) #here I am dropping duplicates,\n",
    "# if fact, one appln_ID could belong to more IPC classes, each mapping on the same Macro Class, with analogous frequence. Thus creating redundant records.\n",
    "\n",
    "# Specify the desktop folder and save the DataFrame to a CSV file\n",
    "desktop_folder = '.\\Dropbox\\Patent name match\\Process Code - Michele'  \n",
    "csv_file_path = f'{desktop_folder}/pbookapplnipc35.csv'\n",
    "pbookappln_ipc35.to_csv(csv_file_path, index=False)\n",
    "pbookappln_ipc35.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce Output pbookcompanypat\n",
    "This table has the Company ID, Appln_ID, Family_ID, Earliest_Filing_Year, Filing Office, Count_References, Count_Citing, Grant_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Applications\n",
    "path_application1 = '.\\Dropbox\\Patent name match\\Patstat2021a\\data_PATSTAT_Global_2021_Autumn\\tls201_part01.zip'\n",
    "csv_application1 = 'tls201_part01.csv'\n",
    "path_application2 = '.\\Dropbox\\Patent name match\\Patstat2021a\\data_PATSTAT_Global_2021_Autumn\\tls201_part02.zip'\n",
    "csv_application2 = 'tls201_part02.csv'\n",
    "path_application3 = '.\\Dropbox\\Patent name match\\Patstat2021a\\data_PATSTAT_Global_2021_Autumn\\tls201_part03.zip'\n",
    "csv_application3 = 'tls201_part03.csv'\n",
    "\n",
    "cols_application = ['appln_id', 'docdb_family_id', 'inpadoc_family_id', 'earliest_filing_year', 'receiving_office']\n",
    "\n",
    "with zipfile.ZipFile(path_application1, 'r') as z:\n",
    "    with z.open(csv_application1) as f:\n",
    "            d_application1 = pd.read_csv(f, nrows = None, usecols=cols_application)\n",
    "\n",
    "with zipfile.ZipFile(path_application2, 'r') as z:\n",
    "    with z.open(csv_application2) as f:\n",
    "            d_application2 = pd.read_csv(f, nrows = None, usecols=cols_application)\n",
    "\n",
    "with zipfile.ZipFile(path_application3, 'r') as z:\n",
    "    with z.open(csv_application3) as f:\n",
    "            d_application3 = pd.read_csv(f, nrows = 500, usecols=cols_application)\n",
    "\n",
    "# concatenate results\n",
    "d_application = pd.concat([d_application1, d_application2, d_application3], axis=0)\n",
    "del d_application1, d_application2, d_application3\n",
    "d_application.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Application-Person Link\n",
    "path_link = '.\\Dropbox\\Patent name match\\Patstat2021a\\data_PATSTAT_Global_2021_Autumn\\tls207_part01.zip'\n",
    "csv_link = 'tls207_part01.csv'\n",
    "cols_link = ['person_id', 'appln_id']\n",
    "\n",
    "with zipfile.ZipFile(path_link, 'r') as z:\n",
    "    with z.open(csv_link) as f:\n",
    "            d_link = pd.read_csv(f, nrows = None, usecols=cols_link)\n",
    "d_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbookcompanypat = pd.merge(disamb_df_fuzzy[['UID', 'applicant_id']], d_link, left_on='applicant_id', right_on='person_id', how='inner').drop('person_id', axis=1)\n",
    "pbookcompanypat = pd.merge(pbookcompanypat, d_application, on='appln_id', how='inner')\n",
    "\n",
    "# Specify the desktop folder and save the DataFrame to a CSV file\n",
    "desktop_folder = '.\\Dropbox\\Patent name match\\Process Code - Michele'  \n",
    "csv_file_path = f'{desktop_folder}/pbookcompanypat.csv'\n",
    "pbookcompanypat.to_csv(csv_file_path, index=False)\n",
    "pbookcompanypat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce Output pbookmatchfinal\n",
    "This table has Company ID, year founded, (year closed), (pbookcy), pbookname, patstat name, type match, (person_ID, patent cy, has patents, type match2). \n",
    "I will use this table to return my results in their entirety. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_merge = disamb_df_fuzzy.loc[:, ['UID', 'applicant_id', 'applicant_name', 'applicant_ctry_code', 'Match Type', 'Score', 'Location Match']]\n",
    "left_merge = df_firms.loc[:, ['UID', 'CompanyName', 'Country/Region', 'Website', 'Year Founded']]\n",
    "pbookmatchfinal = pd.merge(left_merge, right_merge, on='UID', how='inner')\n",
    "\n",
    "# Specify the desktop folder and save the DataFrame to a CSV file\n",
    "desktop_folder = '.\\Dropbox\\Patent name match\\Process Code - Michele'  \n",
    "csv_file_path = f'{desktop_folder}/pbookmatchfinal.csv'\n",
    "pbookmatchfinal.to_csv(csv_file_path, index=False)\n",
    "pbookmatchfinal.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
