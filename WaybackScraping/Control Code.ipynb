{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GreenCatcher Crawler - Fill Excel at Runtime - 15 First Level Links Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "# CHANGE THIS WITH THE PATH WHERE YOU KEEP YOUR wayback_machine_GreenCatcher.py DOCUMENT, DO NOT INSERT THE NAME OF THE DOCUMENT\n",
    "sys.path.append(r'.\\measuring-founding-strategy-main\\crawler') \n",
    "from wayback_machine_GreenCatcher import wayback_machine_GreenCatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(corpus):\n",
    "    try:\n",
    "        text = ' '.join(content for content in corpus)\n",
    "        language_code = detect(text)\n",
    "        return language_code\n",
    "    except:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the wayback machine was first set up in 2001, no snapshot should date before 2000. \n",
    "# So I set it as the date from which I start searching for the earliest snapshot of a website. \n",
    "\n",
    "# set sleeping time in case of extraction error\n",
    "sleep_time = 10\n",
    "\n",
    "# set the first year to start looking for websites from, plus the month and day from which to crawl for the closest snapshot. \n",
    "unprec_year, crawl_month, crawl_day = 2000, 6, 30\n",
    "\n",
    "# CHANGE THIS WITH THE PATH WHERE YOU KEEP YOUR \"FIRMS TO SCRAPE\" EXCEL FILE, INSERT THE NAME OF THE EXCEL FILE AS WELL\n",
    "df = pd.read_excel(r'.\\Firms to Scrape.xlsx')\n",
    "\n",
    "# partition the companies to be scraped so that every three the an excel file with the scraped text is materialized. \n",
    "n = 3\n",
    "\n",
    "for i in range(df.shape[0]//n + 1):\n",
    "\n",
    "    # define the subset of firms which we will scrape in this iteration\n",
    "    start_row = 0 + n*i\n",
    "    end_row = min(n + n*i, df.shape[0])\n",
    "    operative_df = df[start_row:end_row]\n",
    "\n",
    "    # initialize the output dataframe with the columns it will be made from. \n",
    "    output_df = pd.DataFrame(columns=['UID', 'Company', 'YearFounded', 'Website', 'ClosestSnapshot', \n",
    "        'ClosestSnapshotYear', 'TimeStamp', 'Year', 'HomepageText', 'statuscode', 'HTML Language', 'Recognized Language', 'IsValidWebsite'])\n",
    "\n",
    "    for row in tqdm(operative_df.index):\n",
    "\n",
    "        params_got = False\n",
    "        while (params_got == False):\n",
    "            try:\n",
    "                url, founding_year = operative_df.at[row, 'Website'], operative_df.at[row, 'YearFounded']\n",
    "\n",
    "                crawler = wayback_machine_GreenCatcher(url, year_folder = True)\n",
    "\n",
    "                snapshots = crawler.list_snapshots(2000, 1, 1, 2024, 12, 31)\n",
    "                snapshot_timestamps = snapshots['timestamp'].values\n",
    "                earliest_snapshot_year = int(str(min(snapshot_timestamps))[0:4])\n",
    "                latest_snapshot_year = int(str(max(snapshot_timestamps))[0:4])\n",
    "                params_got = True\n",
    "            except Exception as e:\n",
    "                    print('Get Params Failed: {0}'.format(e))\n",
    "                    time.sleep(sleep_time)\n",
    "\n",
    "        for crawl_year in range(max(earliest_snapshot_year, founding_year), latest_snapshot_year + 1):\n",
    "            year_got = False\n",
    "            while (year_got == False):\n",
    "                try:\n",
    "                    sought_date = int(datetime.date(year = crawl_year, month = crawl_month, day = crawl_day).strftime(\"%Y%m%d\") + '000000')\n",
    "                    snapshots['closeness']=np.abs(snapshots['timestamp']-sought_date)\n",
    "                    min_index = snapshots['closeness'].idxmin()\n",
    "                    closest_snapshot_timestamp = snapshots.loc[min_index, 'timestamp']\n",
    "                    closest_snapshot_year = int(str(closest_snapshot_timestamp)[0:4])\n",
    "                    closest_status = snapshots.loc[min_index, 'statuscode']\n",
    "\n",
    "                    temp_result = crawler.crawl_from_date(crawl_year, crawl_month, crawl_day, levels=1, counter_threshold=15)\n",
    "                    homepage_language, corpus = temp_result[0], temp_result[1]\n",
    "\n",
    "                    line_dict = {\n",
    "                        'UID' : operative_df.at[row, 'UID'],\n",
    "                        'Company' : operative_df.at[row, 'Company'],\n",
    "                        'YearFounded' : operative_df.at[row, 'YearFounded'],\n",
    "                        'Website' : operative_df.at[row, 'Website'],\n",
    "                        'ClosestSnapshot' : min(snapshot_timestamps), # ClosestSnapshot in excel is earliest snapshot of wayback machine. \n",
    "                        'ClosestSnapshotYear' : earliest_snapshot_year,\n",
    "                        'TimeStamp' : datetime.datetime.strptime(\n",
    "                        str(closest_snapshot_timestamp), \"%Y%m%d%H%M%S\" # TimeStamp is that of the closest snapshot to the currently scraped year. \n",
    "                        ).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        'Year' : closest_snapshot_year,\n",
    "                        'HomepageText' : corpus,\n",
    "                        'statuscode': closest_status,\n",
    "                        'HTML Language' : homepage_language,\n",
    "                        'Recognized Language' : detect_language(corpus),\n",
    "                        'IsValidWebsite' : crawler.is_valid_url(url)}\n",
    "\n",
    "                    line_df = pd.DataFrame([line_dict.values()], columns=line_dict.keys())\n",
    "                    output_df = pd.concat([output_df, line_df], ignore_index=True)\n",
    "                    year_got = True\n",
    "                except Exception as e:\n",
    "                    print('Get Year Failed: {0}'.format(e))\n",
    "                    time.sleep(sleep_time)\n",
    "    \n",
    "    output_df.to_excel(f'out_summary_{i}.xlsx', index=False, engine='xlsxwriter')\n",
    "\n",
    "print('Iterations Over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
